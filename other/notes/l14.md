**Two slot stdio cache (hypothetical)**

- Reading: each buffer stores cached data for a different 4 KB aligned, 4 KB large file region
  - Once both slots are filled, a cached block is dropped and replaced if hte new seek position isn't within either cached block
- Writing: each buffer stores cached data for a different region being written
  - A block must be flushed to storage when (1) it fills up with 4 KB of write data or (2) if the seek position changes and is no longer the write offset in either cache block
- Approach generalizes to _N_ slots

**Multi slot caches**

- A single slot cache can be effective for many workloads, especially sequential ones
- However, a multi-slot cache often performs better
  - For example, the TLB (page table cache) might often see spatial and temporal locality for
    - The stack page of the current function's stack frame
    - One or more code pages containing code for the current function
    - Possibly one or more heap pages if the function manipulates heap objects
  - Multi slot caches introduce new questions:
    - Associativity: which cache slots can an incoming block map to
    - Eviction: what should get popped out first

**Associativity in a cache with S slots**

- In a fully-associative cache, any block can be stored in any of the S slots
- In a set-associative cache, a block with address A can reside in a specific subset of the slots
- In a direct-mapped cache, each block can only be stored by one cache slot
- The stdio cache is single slot, direct map, and fully associative

**Slot Tags**

- A cache associates each slot with a tag
  - If the slot is empty, the tag is meaningless
  - If the slot is filled, the tag allows the cache to determine if a particular read or write request will be hit for that slot
- For a cache supporting only block-sized, block-aligned reads and writes, the slot tag is just the address of the block to read or write
- The stdio cache:
  - In read mode, the tag is a 4 KB aligned block address (read pulls the aligned block)
  - In write mode, the tag is the current, possibly-unaligned write offset. Write is optimized for sequential writing, so the tag is just the current offset

**Analyzing caches using reference string**

- Many aspects of multi slot cache design are independent of block address semantics, types of underlying storage, etc
- Can abstract away those factors by analyzing cache behavior using a reference string of access patterns
  - Circles mean reading into the cache
- Eviction:
  - Direct mapped caches have a trivial eviction policy - just evict the old block
  - Non direct mapped caches can use a variety of eviction policies
- For any reference string of size S, the optimal eviction policy is one that always chooses to evict the block that won't be needed again for the longest time
  - Figuring this out can be really hard

**Eviction Policies**

- FIFO:
  - evict the block that was fetched the longest time ago
- LFU:
  - least frequently used
  - keep blocks in a linked list. Whenever something is used, put it at the end of the linked list
- LRU:
  - least requently used
  - Evict a block that hasn't been accessed in the recent epoch
  - In each tag, include an LRU bit
- For an N-way associative cache, each cache set is logically a fully-associative cache
- Variants of LRU are most common

**Hit rates v number of cache slots**

- Belady's anomaly: adding more slots does not always increase the hit rate
  - Ex: lecture notes has an example where FIFO gets worse
- LRU is guaranteed to improve the hit rate

**Memory mapped IO**

- With traditional file IO:
  - The kernel's buffer cache stores file blocks in kernel memory
  - System calls like read and write transfer data between user and kernel level buffer cache blocks
  - System calls incur context switch overheads
- With memory mapped IO:
  - Process selects a file to be manipulated using memory mapped IO
  - OS reads the file's data into the buffer cache
  - The OS then configures the process' page table to map the file's buffer cache blocks into the process' address space
  - The process can now read and write the file's data using normal memory operations without using system calls, thereby avoiding context switch overhead

**Files: Random access vs. Stream**

- Linux-like kernels use the file descriptor abstraction to expose many resources (ex: files, raw storage devices, keyboards)
  - random access: has a file position, is seekable, has finite size, and is often available for mmap
  - stream: file does not have a finite position, is not seekable, is not mappable, and may be infinite in size
- Most common example of random access file is a normal file on the SSD
- Stream files are keyboard streams and network streams

**Special files on Linux**

- Linux defines a variety of fake files useful for testing and debugging (or providing dummy files to programs that insist on accepting files as IO)
- `/dev/null` doesn't exist, contains nothing, always returns EOF, any write succeeds without any real effect, discards input: `prog in.txt > /dev/null`
- `/dev/urandom` doesn't exist, contains random data the kernel generates on the fly. A read returns the next bytes of random data, a write adds written bytes to the kernel's entropy pool
- `/dev/zero` doesn't exist, contains all zero bytes, reads return sequences of zeroes, writes always succeed but have no side effect
