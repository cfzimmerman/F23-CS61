**Storage tradeoffs**

- There are physical tradeoffs between:
  - Capacity: how many bytes a device can hold
  - Performance: roughly speaking, how quickly bytes can be read and written
  - Financial cost: how expensive it is to build the device
- Increasing capacity or storage is expensive

**Storage devices: latency and throughput**

- Latency: the _delay_ between a code sending a request and the storage device beginning to read or write an object
- Throughput: the _rate_ at which data can be read or written once the device has begun to handle the request
- An ideal storage device has both low latency and high throughput
  - For storage devices that only store small objects (registers, TLBs, etc), latency is the key metric because not much is being transfered
  - For SSDs with higher capacity, both thoroughput and latency are important

**Access locality**

- A program's access patterns to storage are often predictable
  - Temporal locality: an object that has been accessed recently will likely be accessed in the near future
  - Spatial locality: if an object has been accessed recently, nearby objects are likely to be accessed in the future
- For well behaved programs, we can use a lot of cheap but slow storage and a bit of fast but expensive storage to give the illusion of cheap, big, and fast storage
  - Use caching

**Caching: exploiting locality**

- The cache is a small amount of fast storage that's used to hide the access latencies of larger but slower storage
  - Temporal locality fo object x: once x has been fetched into the cache, subsequent reads and writes of the object can be satisfied by the cache
  - Spatial locality for object x: when a program accesses x, fetch x to nearby objects x + 1, x + 2 into the cache as well so that those objects will already be in the fast storage when the program tries to access them
- Caches:
  - Registers as a cache for RAM
  - TLBs as a cache for Page Table Entries stored in RAM
  - Memory as a cache for SSDs / hard disk

**Caching terminology**

- Block: a unit of data storage; both the cache and underlying storage contain blocks of the same size
- Each block in the underlying storage has an address
- We typically refer to blocks as an integer index treating the underlying storage as an array of blocks
- But blocks typically have a real address on the storage device (ex: memory address in RAM, disk position for a file block)
- A cache contains one or more slots
  - An empty slot contains no cached block
  - A filled slot contains a cached block
- When code tries to read a block, the cache checks whether b is already in the cache
  - If so, that's a cache hit. The block is directly returned, avoiding a slow fetch from the underlying storage
  - If not, there was a cache miss. The block must be pulled from the underlying storage before being returned to the program
- A cache can absorb a write to the block
  - The cache stores the new value for the block but does not synchronously update the underlying storage
  - At some point later, the cache can asychronously flush the new block to the underlying storage
- A cache block is dirty if it has absorbed a write that isn't reflected on the underlying storage
- A cache block is clean otherwise

**Constructing the illusion of fast, big, and cheap storage**

- Modern computers have a storage heirarchy
  - Lower levels provide larger storage capacities
  - Higher levels provide lower latencies and act as chaches, which hide the access costs of reading and writing the lower levels
- L1, L2, L3, and L4 caches sit between CPU and RAM
  - Data is transferred between RAM and the various cache levels in units of a cache line (64 consecutive bytes on a modern x86 chip)
- NUMA: non uniform memory access, not all SSDs can be accessed at the same speed
- Sequential IO is faster than random IO on SSDs, even though SSDs have no mechanical latencies.
  - SSD manufactureres optimize for sequential access simply because people expect it

**Storage: isolation and caching**

- Registers: a CPU's local registers can only be accessed by the code that's currently running on that CPU (no way to read registers across cores)
  - A program is responsible for moving its data between registers and ram. In practice, the compiler usually orchestrates this. OS or in-memory database authors sometimes handle this manually
- Physical RAM: OS uses virtual memory to determine which physical RAm can be accessed by which process. Hardware in L1/L2/L3 caching heirarchy automatically transferes data between RAM and the various caching layers
- Persistent storage: (SSDs and hard drives)
  - The OS provides a file system, using directory + file abstractions to layer a tree-like structure upon raw storage blocks
- Ex:
  - Linux disallows a process from directly interacting with persistent storage devices
  - The Linux kernel maintains a buffer cache in kernel memory
    - The cache stores recently-accessed file blocks
    - Linux does not synchronously flush dirty blocks to the disk unless the application requests it
    - `stdio` provides higher level interfaces for file IO, which invokes system calls internally. `stdio` in libc++ has its own caching layer too
    - stdio file writes are much faster because of many fewer system calls
    - The stdio program might also be less safe, though, because of the caching layer in stdio

**strace**

- `strace` analyzes other programs' system calls
